#PMM 
## Definition
> [!definition] Independence of σ-algebra
> Let $(\Omega, \mathscr{F}, \mathbb{P})$ be a probability space and $\left(\mathscr{G}_i\right)_{i \leqslant n}$ a finite collection of $\sigma$-algebras, $\mathscr{G}_i \subseteq \mathscr{F}$ for $i \leqslant n$. We say that the [[σ-algebra|σ-algebras]] $\left(\mathscr{G}_i\right)_{i \leqslant n}$ are [[independent]] if and only if
> $
> \mathbb{P}\left(A_1 \cap \ldots \cap A_n\right)=\mathbb{P}\left(A_1\right) \cdot \ldots \cdot \mathbb{P}\left(A_n\right), \quad \text { for any } A_i \in \mathscr{G}_i, i \leqslant n .
> $
> For an arbitrary collection $\left(\mathscr{G}_i\right)_{i \in I}$ of sub- $\sigma$-algebras of $\mathscr{F}$, we say that these $\sigma$-algebras are independent if any finite sub-collection of them is.

> [!definition] Independence of Random Variables
> Let $(\Omega, \mathscr{F}, \mathbb{P})$ be a probability space and $\left(X_i\right)_{i \in I}$ a family of random variables with values in some measurable spaces $\left(E_i, \mathscr{E}_i\right)_{i \in I}$. We say that these random variables are [[independent]] if their generated $\sigma$-algebras $\left(\sigma\left(X_i\right)\right)_{i \in I}$ are.
> It follows by the definition that $\left(X_i\right)_{i \in I}$ are independent if and only if for any finite subset $J \subseteq I$
> $
> \mathbb{P}\left(X_i \in A_i \text { for all } i \in J\right)=\prod_{i \in J} \mathbb{P}\left(X_i \in A_i\right), \quad \text { for any } A_i \in \mathscr{E}_i, i \in J \text {. }
> $

### Lemma 3.3
Let $(\Omega, \mathscr{F}, \mathbb{P})$ be a probability space and $A_1, \ldots, A_n$ some events in $\mathscr{F}$. Then, their [[Generated σ-algebra|generated σ-algebras]] are [[independent]] if and only if
$$
\mathbb{P}\left(\bigcap_{i \in J} A_i\right)=\prod_{i \in J} \mathbb{P}\left(A_i\right), \quad \text { for any } J \subseteq\{1, \ldots, n\}
$$
#### Proof
We just show the statement for $n=2$. One direction is obvious. For the other recall that $\sigma(A)=$ $\left\{\Omega, \emptyset, A, A^{\mathrm{c}}\right\}$ and note that if $\mathbb{P}(A \cap B)=\mathbb{P}(A) \mathbb{P}(B)$ then
$$
\mathbb{P}\left(A \cap B^{\mathrm{c}}\right)=\mathbb{P}(A)-\mathbb{P}(A \cap B)=\mathbb{P}(A)(1-\mathbb{P}(B))=\mathbb{P}(A) \mathbb{P}\left(B^{\mathrm{c}}\right)
$$
and the result follows by symmetry.

## Theorem 3.5
Let $(\Omega, \mathscr{F}, \mathbb{P})$ be a probability space, $\left(\mathscr{G}_i\right)_{i \in I}$ an arbitrary collection of $\sigma$-algebras, each generated by [[π-system]] $\mathscr{A}_i \subseteq \mathscr{F}: \mathscr{G}_i=\sigma\left(\mathscr{A}_i\right), i \in I$.Then $\left(\mathscr{G}_i\right)_{i \in I}$ are [[independent]] if and only if
$$
\mathbb{P}\left(\bigcap_i A_i\right)=\prod_{i \in I} \mathbb{P}\left(A_i\right) \quad \text { for any } A_i \in \mathscr{A}_i, i \in J, \text { for any finite subset } J \subseteq I \text {. }
$$

## Theorem 3.7
Let $(\Omega, \mathscr{F}, \mathbb{P})$ be a probability space and $\left(X_i\right)_{i \leqslant n}$ a finite family of random variables with values in some measurable spaces $\left(E_i, \mathscr{E}_i\right)_{i \leqslant n}$. These random variables are [[independent]] in and only if their joint distribution $\mu_{\left(X_1, \ldots, X_n\right)}$ on the product space $\left(\prod_{i \leqslant n} E_i, \times_{i \leqslant n} \mathscr{E}_i\right)$ is the product measure of the marginal distributions $\mu_{X_i}$.

### Corollary 3.8
A sequence $\left(X_n\right)_{n \geqslant 1}$ of real-valued random variables on $(\Omega, \mathscr{F}, \mathbb{P})$ is [[independent]] iff for all $n \geqslant 1$ and all $x_1, \ldots x_n \in \mathbb{R}$ (or $\left.\overline{\mathbb{R}}\right)$,
$$
\mathbb{P}\left(X_1 \leqslant x_1, \ldots, X_n \leqslant x_n\right)=\mathbb{P}\left(X_1 \leqslant x_1\right) \ldots \mathbb{P}\left(X_n \leqslant x_n\right)
$$
## Proposition 3.10
Let $(\Omega, \mathscr{F}, \mathbb{P})$ be a probability space and $\left(X_i\right)_{i \in I}$ a family of [[independent]] random variables with values in some measurable spaces $\left(E_i, \mathscr{E}_i\right)_{i \in I}$ and $f_i: E_i \rightarrow \mathbb{R}$ be measurable, $i \in I$. Then $\left(Y_i:=f_i\left(X_i\right)\right)_{i \in I}$ are [[independent]] random variables.

